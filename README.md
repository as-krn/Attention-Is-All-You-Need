# Attention-Is-All-You-Need

# Transformer Architecture: "Attention Is All You Need" - Comprehensive Guide

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Language](https://img.shields.io/badge/Language-Turkish%20%7C%20English-blue.svg)](#language)

A comprehensive academic analysis and implementation guide of the Transformer architecture from the seminal paper "Attention Is All You Need" by Vaswani et al. (2017).

[🇹🇷 Türkçe](#türkçe) | [🇺🇸 English](#english)

---
<p align="center">
  <img src="https://github.com/user-attachments/assets/757eae9f-587c-4d35-bc1b-26320a4976c6" alt="App Screenshot"/>
</p>


## 🇹🇷 Türkçe

### 📖 Proje Hakkında

Bu proje, Vaswani ve arkadaşları (2017) tarafından yayımlanan "Attention Is All You Need" makalesinde tanıtılan Transformer mimarisinin detaylı akademik incelemesini sunmaktadır. Modern doğal dil işleme (NLP) alanının temelini oluşturan bu mimariyi matematik, kod ve teorik perspektiflerden kapsamlı şekilde ele alır.

### 🎯 İçerik

- **Akademik Analiz**: Transformer mimarisinin matematiksel formalizasyonu
- **Kod İmplementasyonu**: PyTorch ile detaylı kod örnekleri
- **Teorik Açıklama**: Attention mekanizması ve çok başlıklı yapının derinlemesine analizi
- **Pratik Rehber**: Eğitim metodolojisi ve optimizasyon teknikleri
- **Deneysel Sonuçlar**: Orijinal makale sonuçlarının analizi

### 📚 Kapsanan Konular

1. **Attention Mekanizması**
   - Scaled Dot-Product Attention
   - Matematiksel formalizasyon
   - Scaling factor analizi

2. **Multi-Head Attention**
   - Teorik motivasyon
   - Paralel attention hesaplaması
   - Computational efficiency

3. **Pozisyonel Kodlama**
   - Sinusoidal encoding
   - Relative position information
   - Extrapolation capabilities

4. **Encoder-Decoder Mimarisi**
   - Self-attention vs cross-attention
   - Causal masking
   - Residual connections

5. **Eğitim Metodolojisi**
   - Learning rate scheduling
   - Regularization teknikleri
   - Optimization stratejileri


---

## 🇺🇸 English

### 📖 About This Project

This project presents a comprehensive academic analysis of the Transformer architecture introduced in the seminal paper "Attention Is All You Need" by Vaswani et al. (2017). It provides an in-depth examination of this foundational architecture in modern Natural Language Processing (NLP) from mathematical, code, and theoretical perspectives.

### 🎯 Content

- **Academic Analysis**: Mathematical formalization of Transformer architecture
- **Code Implementation**: Detailed code examples with PyTorch
- **Theoretical Explanation**: In-depth analysis of attention mechanism and multi-head structure
- **Practical Guide**: Training methodology and optimization techniques
- **Experimental Results**: Analysis of original paper results

### 📚 Topics Covered

1. **Attention Mechanism**
   - Scaled Dot-Product Attention
   - Mathematical formalization
   - Scaling factor analysis

2. **Multi-Head Attention**
   - Theoretical motivation
   - Parallel attention computation
   - Computational efficiency

3. **Positional Encoding**
   - Sinusoidal encoding
   - Relative position information
   - Extrapolation capabilities

4. **Encoder-Decoder Architecture**
   - Self-attention vs cross-attention
   - Causal masking
   - Residual connections

5. **Training Methodology**
   - Learning rate scheduling
   - Regularization techniques
   - Optimization strategies

