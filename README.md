# Attention-Is-All-You-Need

# Transformer Architecture: "Attention Is All You Need" - Comprehensive Guide

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Language](https://img.shields.io/badge/Language-Turkish%20%7C%20English-blue.svg)](#language)

A comprehensive academic analysis and implementation guide of the Transformer architecture from the seminal paper "Attention Is All You Need" by Vaswani et al. (2017).

[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](#tÃ¼rkÃ§e) | [ğŸ‡ºğŸ‡¸ English](#english)

---
<p align="center">
  <img src="https://github.com/user-attachments/assets/757eae9f-587c-4d35-bc1b-26320a4976c6" alt="App Screenshot"/>
</p>


## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e

### ğŸ“– Proje HakkÄ±nda

Bu proje, Vaswani ve arkadaÅŸlarÄ± (2017) tarafÄ±ndan yayÄ±mlanan "Attention Is All You Need" makalesinde tanÄ±tÄ±lan Transformer mimarisinin detaylÄ± akademik incelemesini sunmaktadÄ±r. Modern doÄŸal dil iÅŸleme (NLP) alanÄ±nÄ±n temelini oluÅŸturan bu mimariyi matematik, kod ve teorik perspektiflerden kapsamlÄ± ÅŸekilde ele alÄ±r.

### ğŸ¯ Ä°Ã§erik

- **Akademik Analiz**: Transformer mimarisinin matematiksel formalizasyonu
- **Kod Ä°mplementasyonu**: PyTorch ile detaylÄ± kod Ã¶rnekleri
- **Teorik AÃ§Ä±klama**: Attention mekanizmasÄ± ve Ã§ok baÅŸlÄ±klÄ± yapÄ±nÄ±n derinlemesine analizi
- **Pratik Rehber**: EÄŸitim metodolojisi ve optimizasyon teknikleri
- **Deneysel SonuÃ§lar**: Orijinal makale sonuÃ§larÄ±nÄ±n analizi

### ğŸ“š Kapsanan Konular

1. **Attention MekanizmasÄ±**
   - Scaled Dot-Product Attention
   - Matematiksel formalizasyon
   - Scaling factor analizi

2. **Multi-Head Attention**
   - Teorik motivasyon
   - Paralel attention hesaplamasÄ±
   - Computational efficiency

3. **Pozisyonel Kodlama**
   - Sinusoidal encoding
   - Relative position information
   - Extrapolation capabilities

4. **Encoder-Decoder Mimarisi**
   - Self-attention vs cross-attention
   - Causal masking
   - Residual connections

5. **EÄŸitim Metodolojisi**
   - Learning rate scheduling
   - Regularization teknikleri
   - Optimization stratejileri


---

## ğŸ‡ºğŸ‡¸ English

### ğŸ“– About This Project

This project presents a comprehensive academic analysis of the Transformer architecture introduced in the seminal paper "Attention Is All You Need" by Vaswani et al. (2017). It provides an in-depth examination of this foundational architecture in modern Natural Language Processing (NLP) from mathematical, code, and theoretical perspectives.

### ğŸ¯ Content

- **Academic Analysis**: Mathematical formalization of Transformer architecture
- **Code Implementation**: Detailed code examples with PyTorch
- **Theoretical Explanation**: In-depth analysis of attention mechanism and multi-head structure
- **Practical Guide**: Training methodology and optimization techniques
- **Experimental Results**: Analysis of original paper results

### ğŸ“š Topics Covered

1. **Attention Mechanism**
   - Scaled Dot-Product Attention
   - Mathematical formalization
   - Scaling factor analysis

2. **Multi-Head Attention**
   - Theoretical motivation
   - Parallel attention computation
   - Computational efficiency

3. **Positional Encoding**
   - Sinusoidal encoding
   - Relative position information
   - Extrapolation capabilities

4. **Encoder-Decoder Architecture**
   - Self-attention vs cross-attention
   - Causal masking
   - Residual connections

5. **Training Methodology**
   - Learning rate scheduling
   - Regularization techniques
   - Optimization strategies

